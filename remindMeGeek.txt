#VI
Replace a word in vi
:%s/FindMe/ReplaceME/g

#GIT
#Résolution de rebase :
git rebase origin/conso
conflit :
git co -- ours file
git add file
git rebase --continue

#Ajout d'un repository sur un remote
#Créer en amont le repo sur le remote
git remote add origin remote repository URL


#Suppression de tag :
en local : git tag -d v2.10.1
en remote : git push --delete origin tagname


#Installer un package .deb
sudo dpkg -i paquet.deb
au besoin : man dpkg

#Installer un rpm
rpm -Uvh pentaho-kettle-5.4.0.5_lbc-1.x86_64.rpm

#ssh & co:
ssh -L  25439:brutus-dwh-db.data.qa.leboncoin.io:5439 sshgw -p 222  -fN
ssh -L  4242:data-redshift-prod:5439  52.18.80.191 -fN
ssh -L  4343:vqa22.leboncoin.lan:5432  vqetl01.leboncoin.lan -fN
ssh -L  7777:vqa23.leboncoin.lan:5432  vqetl01.leboncoin.lan -fN
ssh -L  9090:vvcrm02.leboncoin.lan:3306  vqetl01.leboncoin.lan -fN
ssh -L  8989:127.0.0.1:25   vqetl01.leboncoin.lan -fN #tunnel ssh pour forward le smtp

#Tips Kernel
#Identify the kernel version
uname -a
#Upgrade
#Download linux-headers & linux-image here : http://kernel.ubuntu.com/~kernel-ppa/mainline/
dpkg-i linux-*
reboot

#Connexion au Redshift de prod
psql -U user_sba -h localhost -p 4242 dwhprod

sqlContext.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", s3AccessKey)
sqlContext.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", s3SecretKey)
val store_params_path = "s3a://leboncoin.fr-datalake-prod/platform/prod/databases/blocketdb/public/snapshot/66/store_params/parquet"
val store_param_data = sqlContext.read.parquet(store_params_path)
store_param_data.registerTempTable("store_params")


val generic_store = "s3a://leboncoin.fr-data-staging-prod/work_manager/prod/666/generic/transform_store_master"
sqlContext.read.parquet(generic_store).registerTempTable("store_master")

#Maven
#Création du jar d'un projet
mvn clean package

aws s3 cp stores_sorted_8_1.txt s3://leboncoin.fr-datalake-dev/platform/sba/databases/blocketdb/public/snapshot/8/stores/csv/
aws s3 help

pyspark --jars data/git/data_brutus/spark_etl/target/spark-etl-1.1.0-SNAPSHOT.jar


# PostgreSQL
### Création de rôle
`ALTER database data_em_dev OWNER TO data_em_dev`

`create role data_em_dev
with login password 'data_em_dev' ;`

### Liste des schémas d'une base
\dn


#Elastic
curl -X GET http://elasticsearch01.data.qa.leboncoin.io:9200/platform_qa2/store_master/_mapping?pretty
curl -X POST http://elasticsearch01.data.qa.leboncoin.io:9200/platform_qa2/store_master/_mapping -d @databases/brutus/indexer/platform/mapping_store_master.json?pretty
curl -X GET http://elasticsearch01.data.qa.leboncoin.io:9200/platform_qa2/store_master/_search?q=*&pretty
curl -XGET http://elasticsearch01.data.qa.leboncoin.io:9200/platform_qa2/store_master/_search?q=store_id:987654321
curl -XGET http://elasticsearch01.data.qa.leboncoin.io:9200/platform_qa2/store_master/_search?q=store_id:998885555
curl -X DELETE http://elasticsearch01.data.qa.leboncoin.io:9200/platform_qa2/store_master/987654321

# Generate password
pwgen -0BnC


knife data bag -z show security lbc_brutus  --secret-file ~/.security/encrypted_data_bag_secret


Type tar -xvf yourfile.tar to extract the file to the current directory.
Or tar -C /myfolder -xvf yourfile.tar to extract to another directory.

# Python
Remove compilation files
find . -name *.pyc -type f -delete
